version: incident/v1
id: 94zyw6k5z5k3
name: Intermittent 1Password request failures
impact: minor
systems: []
timeline:
    - ts: "2025-06-16 15:46:07"
      description: We have identified an issue causing intermittent failure requests for US based customers and our engineering team is actively investigating.
    - ts: "2025-06-16 16:06:04"
      description: This issue has been mitigated and we are seeing error levels return back to normal. We will continue to monitor to confirm that the issue has been resolved.
    - ts: "2025-06-16 17:59:29"
      description: This incident has been resolved.
      resolved: true
    - ts: "2025-06-24 13:00:34"
      description: "# Incident Postmortem - Intermittent 500 error failures for US customers.\n\n**Date of Incident:** 2025-06-16   \n**Time of Incident \\(UTC\\):** 14:39 UTC - 17:47 UTC   \n**Service\\(s\\) Affected:** Web Interface, Sign in, Sign up, Admin console, Item Sync, SSO \\(Single Sign On\\), Command Line Interface \\(CLI\\)\\) for US based users.\n\n**Impact Duration:** 68 minutes\n\n## Summary\n\nAt 14:39 UTC, users in the US region experienced intermittent errors while accessing 1Password. The issue stemmed from resource constraints within our infrastructure, specifically affecting the networking services. This was resolved by scaling up the affected services.\n\n## Impact on Customers\n\nDuring the duration of the incident:\n\n* **Web interface, Admin Console:** Customers were able to log in but saw intermittent 500 errors, including “Failed to get Integrations” on the web Interface.\n* **SSO \\(Single Sign On\\), Command Line Interface \\(CLI\\), Item Sync:** There was degraded performance for authentication and API requests.\n* **Sign in, Sign up:** There were intermittent failures on sign in and sign up for some customers during the incident.\n* **Number of Affected Customers \\(approximate\\):** All users accessing the service in the US region were affected.\n* **Geographic Regions Affected \\(if applicable\\):** US\n\n## What Happened?\n\nThe incident began when our internal services started returning errors after deploying the latest version of the 1Password service. As part of the initial investigation, we restarted a supporting network service within our infrastructure, which resulted in an initial recovery of the affected service.\n\n* **Timeline of Events \\(UTC\\):**\n\n    * 2025-06-16 14:39 UTC: Incident Start - Automation detects servers are returning errors\n    * 2025-06-16 14:40 UTC: Initial investigation begins\n    * 2025-06-16 15:21 UTC: Networking updates are rolled out\n    * 2025-06-16 15:23 UTC: Initial service recovery observed\n    * 2025-06-16 15:38 UTC: Root cause identified: Networking applications ran out of allocated resources.\n    * 2025-06-16 15:42 UTC: Additional capacity added to networking applications\n    * 2025-06-17 17:47 UTC: The spike in server errors stopped, and internal monitoring showed that system health had returned to normal.\n    * 2025-06-17 17:53 UTC: Incident resolved\n    \n* **Root Cause Analysis:** An internal service that directs network traffic became resource constrained which caused degraded performance of the service. We first stabilized the system by adding more capacity and have since deployed a permanent fix by increasing system resources to prevent a recurrence.\n\n## How Was It Resolved?\n\n* **Mitigation Steps:** As an immediate mitigation, the number of replicas for the deployment was scaled up.\n* **Resolution Steps:** A more permanent fix was later applied by increasing the allocated resources for the networking applications.\n* **Verification of Resolution:** Around 15:25 UTC, we observed that the spike in 500 errors from the server had completely stopped. The team continued monitoring the errors and confirmed at 17:53 pm EST that allocated resource consumption had been stable for a while.\n\n## What We Are Doing to Prevent Future Incidents\n\n* **Scale  existing resources:** We have effectively scaled resources and resource limits to address additional load and will implement monitoring to ensure we do not hit critical limits\n* **Review and expand existing monitors:** We will review our critical service monitors to improve alerting and catch future incidents earlier, before they have customer impact.\n\n## Next Steps and Communication\n\n* No action is needed from customers\n\nWe are committed to providing a reliable and stable service, and we are taking the necessary steps to learn from this event and prevent it from happening again. Thank you for your understanding.\n\nSincerely,\n\nThe 1Password Team"
