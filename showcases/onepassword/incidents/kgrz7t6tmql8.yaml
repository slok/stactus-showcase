version: incident/v1
id: kgrz7t6tmql8
name: Elevated API Errors
impact: minor
systems: []
timeline:
    - ts: "2025-09-10 21:58:43"
      description: We're experiencing an elevated level of API errors and increased latency, and are currently looking into the issue.
      resolved: true
    - ts: "2025-09-10 21:59:08"
      description: "# Performance Degradation\n\n**Date of Incident:** 2025-09-03  \n**Time of Incident \\(UTC\\):** 11:06 - 12:07  \n**Service\\(s\\) Affected:** All APIs  \n**Impact Duration:** 61 minutes\n\n## Summary\n\nFor 61 minutes on the morning of September 3rd, 2025, all 1Password APIs in the US/Global environment had degraded performance or returned an error for approximately 20% of requests. 92% of the impact was  mitigated within 13 minutes at 11:19 by automation scaling up infrastructure. By 12:06 a manual restart of the remaining infrastructure completed mitigation. A permanent fix was implemented and deployed to prevent the issue from reoccurring.\n\n## Impact on Customers\n\n* **APIs:** High latency, or a 500 Internal Server Error.\n* **Number of Affected Customers:** 20% of all requests returned errors for 13 minutes, 1% thereafter.\n* **Geographic Regions Affected \\(if applicable\\):** 1Password USA/Global\n\n## What Happened?\n\n* **Timeline of Events \\(UTC\\):**\n\n    * 11:05: A customer started a stream of an unusually high volume of requests to an API with sub-optimal performance.\n    * 11:06: Some servers started consuming abnormally high memory, causing slow response times and high error rates.\n    * 11:19: Automation scaled up infrastructure to service additional load\n    * 11:30: Increased errors trigger escalation, on-call engineer begins investigation\n    * 11:51: Engineers declare an incident and alert response teams\n    * 12:02: Response team begins restarting affected servers.\n    * 12:07: All servers completed restarts, and error rates returned to normal levels\n    \n* **Root Cause Analysis:** A poorly performing cache operation was triggered repeatedly in a short period of time across multiple servers, leading directly to greatly delayed responses.\n\n## How Was It Resolved?\n\n* **Mitigation Steps:** Automatic instance scaling restored over 98% of operational capacity after 13 minutes. Full capacity was restored through manual intervention\n* **Resolution Steps:** We refactored the poorly performing query.\n* **Verification of Resolution:** We tested the affected API to confirm refactoring of query produced the desired performance improvement. We deployed the fix and monitored it for 24 hours to assert the issue was resolved.\n\n## What We Are Doing to Prevent Future Incidents\n\n* We are auditing services for sub-optimal query performance.\n\n## Next Steps and Communication\n\n* No action is required from our customers at this time.\n\nâ€Œ\n\nWe are committed to providing a reliable and stable service, and we are taking the necessary steps to learn from this event and prevent it from happening again. Thank you for your understanding.\n\nSincerely,\n\nThe 1Password Team"
