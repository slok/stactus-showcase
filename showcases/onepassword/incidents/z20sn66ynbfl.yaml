version: incident/v1
id: z20sn66ynbfl
name: Degraded performance when accessing 1Password
impact: major
systems:
    - cgltwfnt80mm
    - 44mr9pvxy2h1
    - 5g595mfvxrwb
    - dd7j6hgdy98l
    - nb2s2j09f6sc
    - mxh1682h05hp
    - c1hyq57z8lnq
    - 03vs2j75ds22
    - bqxt4w06fczc
timeline:
    - ts: "2025-09-26 16:51:38"
      description: We are actively investigating an issue where customers may be experiencing degraded performance and slowdowns when accessing 1Password.
      investigating: true
    - ts: "2025-09-26 17:06:43"
      description: The engineering team is restarting key systems in an attempt to alleviate slowdowns.
      investigating: true
    - ts: "2025-09-26 17:25:16"
      description: Overall service is still degraded, but experiencing periods of improvement.
      investigating: true
    - ts: "2025-09-26 17:41:29"
      description: The engineering team is continuing to investigate and actively managing service degradation. Customers may still experience periods of slowdowns when accessing the 1Password service.
      investigating: true
    - ts: "2025-09-26 17:57:04"
      description: The engineering team is actively managing degraded performance and simultaneously attempting to identify root cause. Customers may still experience some slowdowns when accessing 1Password online.
      investigating: true
    - ts: "2025-09-26 18:11:23"
      description: The engineering team is actively managing degraded performance and continuing attempts to identify root cause. Customers may still experience some slowdowns when accessing 1Password online.
      investigating: true
    - ts: "2025-09-26 18:32:51"
      description: Engineering teams have identified a potential cause and are actively managing customer impact. Customers may see some slowdowns, but due to the active management those slowdowns are unlikely.
      investigating: true
    - ts: "2025-09-26 18:44:57"
      description: Engineering teams are testing a mitigation in our non-production environments to verify before rolling it out to our production environments. Customer impact is being actively managed and slowdowns should be rare if they occur at all.
      investigating: true
    - ts: "2025-09-26 18:57:54"
      description: The engineering team has identified the issue and we are continuing to test a mitigation in our test environment before rolling it out to production. Customer impact is being actively managed and slowdowns should be increasingly rare.
    - ts: "2025-09-26 19:13:14"
      description: The engineering team is continuing to test a mitigation in our test environment before deploying it to production. Customer impact is being actively managed and instances of slowdowns should continue to decrease.
    - ts: "2025-09-26 19:28:45"
      description: The engineering team is rolling out a mitigation in our production environment.
    - ts: "2025-09-26 19:53:27"
      description: The engineering team deployed a mitigation, which has addressed the issue. We will continue to monitor performance.
    - ts: "2025-09-26 20:24:58"
      description: This incident has been resolved. We will publish a post-mortem as soon as we complete it.
      resolved: true
    - ts: "2025-10-03 19:24:43"
      description: "# Incident Postmortem - Degraded performance when accessing 1Password\n\n**Date of Incident:** 2025-09-26  \n**Time of Incident:** 4:20pm UTC - 5:39pm UTC  \n**Service\\(s\\) Affected:**  SSO, Web Sign In, Sign Up, Web Interface, CLI  \n**Impact Duration:** ~60 minutes\n\n## Summary\n\nOn September 26, 2025 at 4:20 UTC 1Password’s web interface and APIs experienced degraded performance for all customers in the US region. This was not a result of a security incident and customer data was not affected.\n\n## Impact on Customers\n\nDuring the duration of the incident:\n\n* **Web interface, Administration:** Customers experienced delays when accessing the 1Password web interface.\n* **Single Sign-on \\(SSO\\), Multi-factor Authentication \\(MFA\\):** Users with SSO or MFA enabled experienced delays, and in some cases failures to login.\n* **Command Line Interface \\(CLI\\):** CLI users faced increased latency and timeouts when attempting to access our web APIs.\n* **Browser Extension:** Users requiring web interface authentication experienced delays or failures.\n* **Number of Affected Customers \\(approximate\\):** ~30%\n* **Geographic Regions Affected:** [1password.com](http://1password.com) \\(US/Global\\)\n\n## What Happened?\n\nAt 4:20PM UTC and 5 PM UTC There were traffic bursts which caused extra load on one of our caches. This cache was under-provisioned to handle that spike of activity, which resulted in it exhausting available CPU. This caused cascading errors/latency which manifested in slow and failed requests.\n\n* **Timeline of Events \\(UTC\\):**\n\n    * 2025-09-26 4:20pm: Spike in customer traffic began\n    * 2025-09-26 4:29pm: Automated monitoring detects increased errors and latency\n    * 2025-09-26 4:35pm: The team activates our incident protocol and begins investigation\n    * 2025-09-26 4:58pm: The team decides to restart application servers\n    * 2025-09-26 5:00pm: The servers have been restarted, service is still degraded, as a second traffic burst begins\n    * 2025-09-26 5:18pm: Service starts to improve\n    * 2025-09-26 5:25pm: The team detects increased load for the second time\n    * 2025-09-26 5:33pm: The team restarts application servers again\n    * 2025-09-26 5:39pm: Service is back to normal, team continues to investigate\n    * 2025-09-26 7:26pm: Team has found the issue, and proceeds to upgrade cache instance size\n    * 2025-09-26 7:49pm: Cache upgrade completed successfully\n    * 2025-09-26 7:50pm: Team continues to monitor, performance has returned to nominal levels\n    * 2025-09-26 8:24pm: Incident is marked as resolved\n    \n* **Root Cause Analysis:**\n\n    A code library installed in July introduced latency issues for cache connections. Authentication operations weren't properly rate-limited, allowing large traffic influxes. During peak traffic periods, the cache infrastructure was operating near maximum CPU capacity. The incident occurred when a burst of authentication traffic pushed the cache CPU utilization to 100%. The increased latency and CPU usage together directly caused the incident.\n\n\n* **Contributing Factors:**\n\n    * Latency increase due to cache library version upgrade\n    * Inadequate rate limiting allowed traffic bursts to go unchecked\n    * Cache instance size is under-provisioned\n    \n\n## How Was It Resolved?\n\n* **Mitigation Steps:** Restarting application servers temporarily mitigated the latency and errors, but the problems returned when traffic spiked again.\n* **Resolution Steps:** Increasing the instance size for the cache resolved the issue.\n* **Verification of Resolution:** The incident team tested the upgrade in a staging deployment before executing it in production. They then monitored metrics to confirm the system returned to normal levels.\n\n## What We Are Doing to Prevent Future Incidents\n\n* **Improve capacity planning for cache:** We will ensure our internal infrastructure is properly sized to handle current traffic volumes and accommodate future growth. We'll implement regular resource evaluations to maintain adequate capacity as our traffic increases. We will also implement proactive alerting systems that notify our teams when resource utilization approaches critical thresholds.\n* **Update library to a more performant version:** We will upgrade our caching library to the latest stable version to eliminate the current latency issues.\n* **Improve rate limiting for operations that triggered the traffic burst:** Enhancing our rate limiting system will significantly improve our ability to handle future traffic bursts.\n* **Timeline for Implementation:** Observability improvements have already been implemented, and we will complete the remaining work by the end of Q1, 2026.\n\n## Next Steps and Communication\n\nNo action is required from our customers at this time.\n\n‌\n\nWe are committed to providing a reliable and stable service, and we are taking the necessary steps to learn from this event and prevent it from happening again. Thank you for your understanding.\n\nSincerely,\n\nThe 1Password Team"
