version: incident/v1
id: lgnmptjvcl5g
name: Disruption with some GitHub services
impact: minor
systems:
    - kr09ddfgbfsf
    - br0l2tvcx85d
timeline:
    - ts: "2025-08-19 13:39:53"
      description: We are currently investigating this issue.
      investigating: true
    - ts: "2025-08-19 13:39:59"
      description: Issues with timeouts when searching
      investigating: true
    - ts: "2025-08-19 13:44:54"
      description: Issues is experiencing degraded performance. We are continuing to investigate.
      investigating: true
    - ts: "2025-08-19 13:45:36"
      description: Actions is experiencing degraded performance. We are continuing to investigate.
      investigating: true
    - ts: "2025-08-19 14:11:29"
      description: We are seeing slightly elevated latency on some Issues endpoints and searches for workflow runs in Actions may not return quickly.
      investigating: true
    - ts: "2025-08-19 14:45:04"
      description: We were able to mitigate the slowness by throttling some search indexing and will work on the issues created by the increased search indexing so they do not have latency impact.
      investigating: true
    - ts: "2025-08-19 14:46:52"
      description: Issues is operating normally.
      investigating: true
    - ts: "2025-08-19 14:46:55"
      description: Actions is operating normally.
      investigating: true
    - ts: "2025-08-19 14:46:58"
      description: On August 19, 2025, between 13:35 UTC and 14:33 UTC, GitHub search was in a degraded state. When searching for pull requests, issues, and workflow runs, users would have seen some slow, empty or incomplete results. In some cases, pull requests failed to load.<br /><br />The incident was triggered by intermittent connectivity issues between our load balancers and search hosts. While retry logic initially masked these problems, retry queues eventually overwhelmed the load balancers, causing failure. The incident was mitigated at 14:33 UTC by throttling our search index pipeline. <br /><br />Our automated alerting and internal retries reduced the impact of this event significantly. As a result of this incident we believe we have identified a faster way to mitigate it in the future. We are also working on multiple solutions to resolve the underlying connectivity issues.<br />
      resolved: true
