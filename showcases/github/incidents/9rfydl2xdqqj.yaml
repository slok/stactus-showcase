version: incident/v1
id: 9rfydl2xdqqj
name: Incident with search on GitHub we are seeing increased failure rates
impact: major
systems:
    - brv1bkgrwx7q
    - kr09ddfgbfsf
    - hhtssxt0f5v2
    - br0l2tvcx85d
    - st3j38cctv9l
timeline:
    - ts: "2025-08-12 14:12:10"
      description: We are investigating reports of degraded performance for API Requests, Actions, Issues and Pull Requests
      investigating: true
    - ts: "2025-08-12 14:30:14"
      description: Packages is experiencing degraded performance. We are continuing to investigate.
      investigating: true
    - ts: "2025-08-12 14:53:43"
      description: We are investigating reports of degraded performance in services backed by search. The team continues to investigate why requests are failing to reach our search clusters.
      investigating: true
    - ts: "2025-08-12 15:20:18"
      description: We are experiencing increased latency in our API layers and inconsistently degraded experiences when loading or querying issues, pull requests, labels, packages, releases, workflow runs, projects, and repositories, among others. Investigation is underway.
      investigating: true
    - ts: "2025-08-12 15:48:53"
      description: We are seeing partial recovery in service availability, but still see inconsistent experiences and stale search data across services. Investigation and mitigations are underway.
      investigating: true
    - ts: "2025-08-12 16:33:42"
      description: Service availability has been mostly restored, but increased load/query latency and stale search results persist. We continue to work towards full mitigation.
      investigating: true
    - ts: "2025-08-12 17:07:42"
      description: Service availability has been mostly restored, but some users will continue to see increased request latency and stale search results. We are still working towards full recovery.
      investigating: true
    - ts: "2025-08-12 17:56:13"
      description: On August 12, 2025, between 13:30 UTC and 17:14 UTC, GitHub search was in a degraded state. Users experienced inaccurate or incomplete results, failures to load certain pages (like Issues, Pull Requests, Projects, and Deployments), and broken components like Actions workflow and label filters.<br /><br />Most user impact occurred between 14:00 UTC and 15:30 UTC, when up to 75% of search queries failed, and updates to search results were delayed by up to 100 minutes. <br /><br />The incident was triggered by intermittent connectivity issues between our load balancers and search hosts. While retry logic initially masked these problems, retry queues eventually overwhelmed the load balancers, causing failure. The query failures were mitigated at 15:30 UTC after throttling our search indexing pipeline to reduce load and stabilize retries.  The connectivity failures were resolved at 17:14 UTC after the automated reboot of a search host, causing the rest of the system to recover.  <br /><br />We have improved internal monitors and playbooks, and tuned our search cluster load balancer to further mitigate the recurrence of this failure mode. We continue to invest in resolving the underlying connectivity issues.
      resolved: true
