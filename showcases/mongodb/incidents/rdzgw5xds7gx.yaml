version: incident/v1
id: rdzgw5xds7gx
name: Metrics ingestion delays and delayed cluster operations
impact: minor
systems:
    - 98lbmdz85yp0
timeline:
    - ts: "2025-10-07 13:42:48"
      description: We are currently investigating a delay in our metric ingestion pipeline.  Customers may see delays in cluster operations.
      investigating: true
    - ts: "2025-10-07 15:45:07"
      description: We are continuing to investigate the issue. During this time Host Down alerts will not fire and metrics may be missing or delayed for Atlas and Cloud Manager Clusters. Cluster operations may also be delayed.
      investigating: true
    - ts: "2025-10-07 17:44:40"
      description: We have identified the cause of the issue and have taken corrective actions. We are monitoring the impact of those mitigations. During this time Host Down alerts, metrics, and some Atlas Cluster Operations may still be delayed or missing.
    - ts: "2025-10-07 19:32:12"
      description: A fix has been implemented and we are monitoring the results.
    - ts: "2025-10-07 20:03:22"
      description: This incident has been resolved.
      resolved: true
    - ts: "2025-10-13 15:04:04"
      description: "## Executive Summary\n\n**Incident Date/Time**: October 6–7, 2023  \n**Duration**: Approximately 2 days \\(partial impact observed intermittently over this period\\)  \n**Impact**:\n\n* Delays in metrics ingestion impacting the timeliness of operational data displayed in MongoDB Atlas dashboards.\n* Temporary degradation of Atlas Cluster management operations due to backend system strain.\n* No disruptions to the health, availability, or data integrity of customer clusters.\n\n**Root Cause**: Increased load on an internal backing database servicing Atlas and Cloud Manager monitoring systems due to a combination of unsharded high-traffic collections concentrated on a single shard, inefficient query patterns, and spikes in resource consumption coinciding with a software rollout.\n\n**Status**: Resolved\n\n## What Happened\n\nOn October 6 and 7, MongoDB Atlas and Cloud Manager encountered temporary delays in metrics ingestion and backend disruptions affecting certain operational workflows. The primary contributors were elevated resource consumption and localized data distribution challenges in an internal database cluster supporting critical monitoring and operational systems.\n\nInitial investigations pointed to high resource usage in one shard of the backing database cluster. However, further review revealed systemic inefficiencies, including:\n\n* Unsharded high-traffic collections leading to uneven data distribution across shards.\n* Inefficient query patterns, such as full collection scans, amplifying strain on high-load shards.\n* Elevated resource consumption during the rollout of an updated internal software version, increasing system load beyond anticipated thresholds.\n\nThough the functionality and availability of customer clusters remained unaffected, customers experienced degraded monitoring performance and fewer timely Atlas dashboard updates. MongoDB implemented mitigation measures to stabilize the system and then resolved long-term root causes to restore operational workflows.\n\n## Impact Assessment\n\nAffected Services:\n\n* Metrics ingestion \\(monitoring performance\\).\n* Atlas Cluster Management Operations \\(temporary delays\\).\n\nCustomer Impact:\n\n* Delayed metrics ingestion within Atlas dashboards limited real-time visibility into operational data.\n* Temporary delays in Atlas cluster management operations such as provisioning, resizing, and other backend workflows.\n* No loss of data or disruption to customer application availability or performance.\n\n## Root Cause Analysis\n\nThe incident resulted from several contributing factors:\n\n* High-traffic collections concentrated on a single shard led to hotspots within the internal database cluster during periods of elevated load.\n* Inefficient query designs placed additional strain on the impacted shard during standard operations.\n* The rollout of an updated software generated a temporary spike in resource demands, exacerbating load-related challenges in the cluster.\n\nCombined, these factors overwhelmed the targeted shard and contributed to backend delays affecting metrics ingestion and operational requests.\n\n## Prevention\n\nMongoDB has identified several lasting improvements and implemented strategic fixes to prevent recurrence:\n\n1. Collections experiencing concentrated load will be sharded to distribute traffic more evenly across multiple nodes, alleviating pressure on single shards.\n2. Inefficient queries are being optimized to improve resource utilization and reduce latency during routine operations.\n3. Additional infrastructure capacity has been provisioned to better handle elevated traffic volumes. Capacity planning processes are also being refined to anticipate future spikes in load.\n4. Processes for deploying updated versions of software are being redesigned to account for predictable increases in system resource demands during rollouts, ensuring smoother deployment.\n\n## Next Steps\n\n* MongoDB Engineering teams will continue monitoring the performance of sharding strategies and query optimizations to ensure effective resolution of hotspots.\n* Updated capacity models will incorporate metrics from this event to strengthen proactive planning across the Atlas platform.\n* Feedback mechanisms for detecting elevated load conditions will be further expanded to provide faster anomaly detection and response.\n\n## Conclusion\n\nWe apologize for the impact of this event on our customers. We are aware that this outage had an impact on our customer’s operations. MongoDB’s highest priorities are security, durability, availability, and performance. We are committed to learning from this event and to update our internal processes to prevent similar scenarios in the future."
